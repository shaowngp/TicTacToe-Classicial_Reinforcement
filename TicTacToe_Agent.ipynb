{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rznSDgbvGggG"
   },
   "source": [
    "## Assignment: Tic-Tac-Toe\n",
    "\n",
    "### Tic-Tac-Toe Agent\n",
    "​\n",
    "In this notebook, I'll build an RL agent (using Q-learning) that learns to play Numerical Tic-Tac-Toe with odd numbers. The environment is playing randomly with the agent, i.e. its strategy is to put an even number randomly in an empty cell. The following is the layout of the notebook:\n",
    "\n",
    "        - Defining epsilon-greedy strategy \n",
    "        - Tracking state-action pairs for convergence \n",
    "        - Defining hyperparameters for the Q-learning algorithm \n",
    "        - Generating episode and applying Q-update equation \n",
    "        - Checking convergence in Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-30 23:11:05.652111\n"
     ]
    }
   ],
   "source": [
    "#!pip install gym\n",
    "from datetime import datetime\n",
    "time1=datetime.today()\n",
    "print(time1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries\n",
    "Writing the code to import Tic-Tac-Toe class from the environment file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6SFNYceFGggJ"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from TCGame_Env import TicTacToe\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wYLQyopEG8nz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHAOWN\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Function to convert state array into a string to store it as keys in the dictionary\n",
    "# states in Q-dictionary will be of form: x-4-5-3-8-x-x-x-x\n",
    "#   x | 4 | 5\n",
    "#   ----------\n",
    "#   3 | 8 | x\n",
    "#   ----------\n",
    "#   x | x | x\n",
    "\n",
    "def Q_state(state):\n",
    "    return ('-'.join(str(e) for e in state)).replace('nan','x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZebMOoiVHBBr"
   },
   "outputs": [],
   "source": [
    "# Defining a function which will return valid (all possible actions) actions corresponding to a state\n",
    "\n",
    "def valid_actions(state):\n",
    "\n",
    "    valid_Actions = []\n",
    "    \n",
    "    valid_Actions = [i for i in env.action_space(state)[0]]\n",
    "    return valid_Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRciPUkYHDWf"
   },
   "outputs": [],
   "source": [
    "# Defining a function which will add new Q-values to the Q-dictionary. \n",
    "def add_to_dict(state):\n",
    "    state1 = Q_state(state)\n",
    "    \n",
    "    valid_act = valid_actions(state)\n",
    "    if state1 not in Q_dict.keys():\n",
    "        for action in valid_act:\n",
    "            Q_dict[state1][action]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fNNi_EfHGggM"
   },
   "source": [
    "#### Epsilon-greedy strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0lMfqiJGggN"
   },
   "outputs": [],
   "source": [
    "# Defining epsilon-greedy policy.\n",
    "def epsilon_greedy_policy(state, time):\n",
    "    \n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*time)\n",
    "    rand = np.random.random()       \n",
    "    \n",
    "    if rand > epsilon:\n",
    "        action = max(Q_dict[Q_state(state)],key=Q_dict[Q_state(state)].get)\n",
    "    else:\n",
    "        action = random.sample(valid_actions(state),1)[0]   \n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H2kyQHOMGggR"
   },
   "source": [
    "#### Tracking the state-action pairs for checking convergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qcxZ29vdGggS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Initialising Q_dictionary as 'Q_dict' and States_tracked as 'States_track' (for convergence)\n",
    "Q_dict = collections.defaultdict(dict)\n",
    "\n",
    "States_track = collections.defaultdict(dict)\n",
    "\n",
    "## For retraining with already trained Policy learned\n",
    "# with open('Policy.pkl', 'rb') as handle:\n",
    "#     Q_dict = pickle.load(handle)\n",
    "      \n",
    "\n",
    "# with open('States_tracked.pkl', 'rb') as handle:\n",
    "#     States_track = pickle.load(handle)    \n",
    "    \n",
    "print(len(Q_dict))\n",
    "print(len(States_track))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vs73iv8fHOxV"
   },
   "outputs": [],
   "source": [
    "# Initialising few random states to be tracked\n",
    "def initialise_tracking_states():\n",
    "    sample_state_q_val = [('x-3-x-x-x-6-x-x-x',(0,1)), ('x-1-x-x-x-x-8-x-x',(2,9)),\n",
    "                          ('x-x-x-x-6-x-x-x-5',(2,7)), ('x-x-x-x-9-x-6-x-x',(1,7)),\n",
    "                          ('x-5-x-2-x-x-4-7-x',(0,9)), ('9-x-5-x-x-x-8-x-4',(1,3)),\n",
    "                          ('2-7-x-x-6-x-x-3-x',(8,5)), ('9-x-x-x-x-2-x-x-x',(2,5)),\n",
    "                          ('x-x-7-x-x-x-x-x-2',(1,5)), ('5-x-x-x-x-6-x-x-x',(4,9)),\n",
    "                          ('4-x-x-6-x-x-3-1-x',(8,5)), ('5-x-8-x-x-6-3-x-x',(3,1)),\n",
    "                          ('x-6-5-x-2-x-x-3-x',(0,7)), ('7-x-5-x-2-x-x-x-6',(1,3))]\n",
    "    \n",
    "    for q_val in sample_state_q_val:\n",
    "        state = q_val[0]\n",
    "        action = q_val[1]\n",
    "        States_track[state][action] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dAbwJDMVHpwl"
   },
   "outputs": [],
   "source": [
    "# Defining a function to save the Q-dictionary as a pickle file\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Pyj7nMVHsBi"
   },
   "outputs": [],
   "source": [
    "# Defining a function to track the states initialized\n",
    "def save_tracking_states():\n",
    "    for state in States_track.keys():\n",
    "        for action in States_track[state].keys():\n",
    "            if state in Q_dict and action in Q_dict[state]:\n",
    "                States_track[state][action].append(Q_dict[state][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_8xSluUHvew"
   },
   "outputs": [],
   "source": [
    "initialise_tracking_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-iPt--E9GggV"
   },
   "source": [
    "#### Defining hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G0_f5czFGggW"
   },
   "outputs": [],
   "source": [
    "EPISODES = 4000000\n",
    "LR = 0.01\n",
    "GAMMA = 0.9\n",
    "threshold = 2500\n",
    "checkpoint_threshold = 400000\n",
    "max_epsilon = 1.0             \n",
    "min_epsilon = 0.001           \n",
    "decay_rate = 0.000002         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Md6twJ7wGggh"
   },
   "source": [
    "### Q-update loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ldCgQuDNGggj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████                                                                | 400236/4000000 [03:54<30:56, 1939.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Till now agent has won : 0.268, environment has won : 0.275 and tie : 0.457 in  400000 number of episodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████▏                                                        | 800312/4000000 [07:12<24:37, 2165.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Till now agent has won : 0.302, environment has won : 0.260 and tie : 0.439 in  800000 number of episodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████                                                 | 1200254/4000000 [10:15<21:02, 2216.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Till now agent has won : 0.336, environment has won : 0.247 and tie : 0.417 in  1200000 number of episodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████                                          | 1600393/4000000 [12:56<14:47, 2703.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Till now agent has won : 0.412, environment has won : 0.218 and tie : 0.370 in  1600000 number of episodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████                                   | 2000621/4000000 [15:15<11:03, 3012.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Till now agent has won : 0.507, environment has won : 0.181 and tie : 0.312 in  2000000 number of episodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████                            | 2400387/4000000 [17:32<08:56, 2978.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Till now agent has won : 0.576, environment has won : 0.154 and tie : 0.269 in  2400000 number of episodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████                     | 2800581/4000000 [19:46<06:50, 2922.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Till now agent has won : 0.627, environment has won : 0.135 and tie : 0.238 in  2800000 number of episodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████              | 3200465/4000000 [22:00<04:21, 3057.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Till now agent has won : 0.666, environment has won : 0.119 and tie : 0.215 in  3200000 number of episodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████       | 3600359/4000000 [24:16<02:13, 2991.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Till now agent has won : 0.696, environment has won : 0.108 and tie : 0.196 in  3600000 number of episodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████████████████████████████████████████████████████████████████▎| 3959020/4000000 [26:18<00:13, 2965.24it/s]"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "tracker_q={}\n",
    "tracker_q['x-3-x-x-x-6-x-x-x']=[]\n",
    "tracker_q['x-1-x-x-x-x-8-x-x']=[]\n",
    "tracker_q['x-x-x-x-6-x-x-x-5']=[]\n",
    "tracker_q['x-x-x-x-9-x-6-x-x']=[]\n",
    "tracker_q['x-5-x-2-x-x-4-7-x']=[]\n",
    "tracker_q['9-x-5-x-x-x-8-x-4']=[]\n",
    "tracker_q['2-7-x-x-6-x-x-3-x']=[]\n",
    "tracker_q['9-x-x-x-x-2-x-x-x']=[]\n",
    "tracker_q['x-x-7-x-x-x-x-x-2']=[]\n",
    "tracker_q['5-x-x-x-x-6-x-x-x']=[]\n",
    "tracker_q['4-x-x-6-x-x-3-1-x']=[]\n",
    "tracker_q['5-x-8-x-x-6-3-x-x']=[]\n",
    "tracker_q['x-6-5-x-2-x-x-3-x']=[]\n",
    "tracker_q['7-x-5-x-2-x-x-x-6']=[]   \n",
    "                    \n",
    "num_of_agent_win = 0\n",
    "num_of_env_win = 0\n",
    "num_of_tie = 0\n",
    "\n",
    "for episode in tqdm(range(EPISODES)):\n",
    "    \n",
    "    env = TicTacToe()\n",
    "    \n",
    "    curr_state = env.state\n",
    "    reward=0\n",
    "    total_reward = 0\n",
    "    add_to_dict(curr_state)\n",
    "    in_terminal = False\n",
    "    \n",
    "    \n",
    "    while not(in_terminal):\n",
    "        curr_action = epsilon_greedy_policy(curr_state, episode)\n",
    "    \n",
    "        if Q_state(curr_state) in tracker_q.keys():\n",
    "            tracker_q[Q_state(curr_state)].append(curr_action)\n",
    "\n",
    "        next_state,reward,in_terminal, msg = env.step(curr_state,curr_action) \n",
    "\n",
    "        curr_lookup = Q_state(curr_state)\n",
    "        next_lookup = Q_state(next_state)\n",
    "\n",
    "        if in_terminal:\n",
    "            q_value_max = 0\n",
    "            \n",
    "            \n",
    "            if msg == \"Agent has won!\":\n",
    "                num_of_agent_win += 1\n",
    "                \n",
    "            elif msg == \"Environment has won!\":\n",
    "                num_of_env_win += 1\n",
    "                \n",
    "            else:\n",
    "                num_of_tie += 1\n",
    "                \n",
    "        else:\n",
    "            add_to_dict(next_state)\n",
    "            max_next = max(Q_dict[next_lookup],key=Q_dict[next_lookup].get)\n",
    "            q_value_max = Q_dict[next_lookup][max_next]\n",
    "\n",
    "        Q_dict[curr_lookup][curr_action] += LR * ((reward + (GAMMA * (q_value_max))) - Q_dict[curr_lookup][curr_action]) \n",
    "        \n",
    "        curr_state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    if (episode + 1) % checkpoint_threshold == 0:\n",
    "        print(\"Till now agent has won : %.3f, environment has won : %.3f and no. of tie : %.3f in  %d number of episodes\"% ( \n",
    "            num_of_agent_win / (episode + 1), \n",
    "            num_of_env_win /(episode + 1), \n",
    "            num_of_tie / (episode + 1), \n",
    "            episode + 1))\n",
    "\n",
    "    if ((episode + 1) % threshold) == 0:   \n",
    "        save_tracking_states()\n",
    "\n",
    "           \n",
    "elapsed_time = time.time() - start_time\n",
    "print('Total elapsed time: ', elapsed_time)\n",
    "\n",
    "save_obj(States_track,'States_tracked')\n",
    "save_obj(Q_dict,'Policy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhdWewc4Gggo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LfSgVuHGggu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6eMFbb8Ggg2"
   },
   "source": [
    "#### Check the Q-dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fr9d2fcVGgg4"
   },
   "outputs": [],
   "source": [
    "Q_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1tnDJWkGgg9"
   },
   "outputs": [],
   "source": [
    "len(Q_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KGPZEQDFGghG"
   },
   "source": [
    "#### Checking the states tracked for Q-values convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9s1Tvz8HGghH"
   },
   "outputs": [],
   "source": [
    "# Writing the code for plotting the graphs for state-action pairs tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pVQInsg7GghL"
   },
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(25,15))\n",
    "plt.subplot(241)\n",
    "t1=States_track['9-x-x-x-x-2-x-x-x'][(2,5)] \n",
    "plt.title(\"Convergence plot for: ('9-x-x-x-x-2-x-x-x',(2,5))\")\n",
    "plt.plot(np.asarray(range(0, len(t1))),np.asarray(t1))\n",
    "\n",
    "plt.subplot(242)\n",
    "t2=States_track['x-x-x-x-6-x-x-x-5'][(2,7)]\n",
    "plt.title(\"Convergence plot for: ('x-x-x-x-6-x-x-x-5',(2,7))\")\n",
    "plt.plot(np.asarray(range(0, len(t2))),np.asarray(t2))\n",
    "\n",
    "plt.subplot(243)\n",
    "t3=States_track['x-x-7-x-x-x-x-x-2'][(1,5)]\n",
    "plt.title(\"Convergence plot for: ('x-x-7-x-x-x-x-x-2',(1,5))\")\n",
    "plt.plot(np.asarray(range(0, len(t3))),np.asarray(t3))\n",
    "\n",
    "plt.subplot(244)\n",
    "t4=States_track['x-3-x-x-x-6-x-x-x'][(0,1)]\n",
    "plt.title(\"Convergence plot for: ('x-3-x-x-x-6-x-x-x',(0,1))\")\n",
    "plt.plot(np.asarray(range(0, len(t4))),np.asarray(t4))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2Opp8_NITkC"
   },
   "source": [
    "### Epsilon - decay checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gQ_D_JsuGghR"
   },
   "outputs": [],
   "source": [
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.001\n",
    "time = np.arange(0,4000000)\n",
    "epsilon = []\n",
    "for i in range(0,4000000):\n",
    "    epsilon.append(min_epsilon + (max_epsilon - min_epsilon) * np.exp(-0.000001*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "J7c2xADQGghV",
    "outputId": "cb60fce3-570b-45fb-bd83-abde3d13b273"
   },
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "59BRf43IJiQ1"
   },
   "outputs": [],
   "source": [
    "time2=datetime.today()\n",
    "print(' Total Elapsed time: '+ str(time2-time1))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TicTacToe_Agent.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
